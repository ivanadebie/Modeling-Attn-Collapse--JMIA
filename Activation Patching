from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
ROOT = "/content/drive/MyDrive/attention-collapse-llm"
STEP5 = f"{ROOT}/step5"
OUT   = f"{ROOT}/step5_outputs"
DATA  = f"{ROOT}/data"
os.makedirs(STEP5, exist_ok=True)
os.makedirs(OUT, exist_ok=True)
os.makedirs(DATA, exist_ok=True)
print("Folders ready:", ROOT)


# Clean & install compatible versions
!pip -q uninstall -y transformers torchvision torchaudio > /dev/null 2>&1
!pip -q install --upgrade "torch==2.3.1" "torchvision==0.18.1" --index-url https://download.pytorch.org/whl/cu121
!pip -q install "transformer-lens==2.7.0" "einops==0.8.1" "transformers==4.41.2"
print("Installed torch/torchvision/transformer-lens/transformers")


import os, pandas as pd

paths = {
    "shortlist_csv": f"{OUT}/step5_head_shortlist.csv",
    "plan_csv": f"{OUT}/step5_ablation_plan.csv",
    "examples_csv": f"{STEP5}/merged_predictions_metrics.csv",
    "per_head_csv": f"{DATA}/gpt2small_attn_metrics_per_head.csv",
    "qa_csv": f"{STEP5}/Distractors - Sheet1.csv"
}
print("Checking files:")
for k,p in paths.items():
    print(k, "->", os.path.exists(p), "|", p)


import json, os

CFG = {
  "paths": {
    "shortlist_csv": f"{OUT}/step5_head_shortlist.csv",
    "plan_csv": f"{OUT}/step5_ablation_plan.csv",
    "examples_csv": f"{STEP5}/merged_predictions_metrics.csv",
    "per_head_csv": f"{DATA}/gpt2small_attn_metrics_per_head.csv",
    "output_csv": f"{OUT}/step5_ablation_results.csv"
  }
}
with open(f"{STEP5}/step5_ablation_config.json","w") as f:
    json.dump(CFG, f, indent=2)

print("Config written:", f"{STEP5}/step5_ablation_config.json")
print(json.dumps(CFG, indent=2))


import pandas as pd

qa = pd.read_csv(f"{STEP5}/Distractors - Sheet1.csv")
assert "Question" in qa.columns and "Gold Text" in qa.columns, "QA sheet must have 'Question' and 'Gold Text'."

gold_map = qa.rename(columns={"Question":"question","Gold Text":"gold_answer"})[["question","gold_answer"]].copy()
gold_map["question"] = gold_map["question"].astype(str).str.strip()
gold_map["gold_answer"] = gold_map["gold_answer"].astype(str).str.strip()

ex = pd.read_csv(f"{STEP5}/merged_predictions_metrics.csv")
ex["question"] = ex["question"].astype(str).str.strip()

merged = ex.merge(gold_map, on="question", how="left")
merged.to_csv(f"{STEP5}/merged_predictions_metrics.csv", index=False)

print("Merged gold answers into examples")
print("rows:", len(merged), " | missing gold:", merged["gold_answer_x"].isna().sum())
print(merged[["row_id","question"]].head(3))


runner_code = r'''
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json
os.environ["TRANSFORMERS_NO_TORCHVISION"] = "1"  # avoid torchvision import issues

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple

CFG = json.load(open("step5_ablation_config.json","r"))

SHORTLIST = Path(CFG["paths"]["shortlist_csv"])
PLAN      = Path(CFG["paths"]["plan_csv"])
EXAMPLES  = Path(CFG["paths"]["examples_csv"])
PER_HEAD  = Path(CFG["paths"]["per_head_csv"])
OUTPUT    = Path(CFG["paths"]["output_csv"])

short_df = pd.read_csv(SHORTLIST)   # layer, head, (_collapse_score)
plan_df  = pd.read_csv(PLAN)        # first col = row_id
ex_df    = pd.read_csv(EXAMPLES)    # row_id, question, ctxs, gold_answer, ...

TARGETS: List[Tuple[int,int]] = [(int(r["layer"]), int(r["head"])) for _, r in short_df.iterrows()]

# ---- Model ----
from transformer_lens import HookedTransformer
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model  = HookedTransformer.from_pretrained("gpt2-small", device=device)

ALPHA_DOSAGE = 0.5  # try 0.5, then 0.25 for sensitivity

def make_ablation_hooks(targets: List[Tuple[int,int]]):
    """Scale specified heads at attn.hook_result by ALPHA_DOSAGE (0..1)."""
    by_layer = {}
    for L, H in targets:
        by_layer.setdefault(int(L), []).append(int(H))
    hooks = []
    for L, heads in by_layer.items():
        name = f"blocks.{L}.attn.hook_result"
        def _mk(hlist):
            def _fn(value, hook):
                value[:, hlist, :, :] = value[:, hlist, :, :] * ALPHA_DOSAGE
                return value
            return _fn
        hooks.append((name, _mk(heads)))
    return hooks

ABLT_HOOKS = make_ablation_hooks(TARGETS)

def build_prompt(row: pd.Series) -> str:
    q   = str(row.get("question","")).strip()
    ctx = str(row.get("ctxs","")).strip()
    return (
        "Answer ONLY using the context. If the answer is not explicitly in the context, reply exactly: I don't know.\n"
        "Return a SHORT answer (1–6 words), no extra text.\n\n"
        f"Context:\n{ctx}\n\nQuestion: {q}\nAnswer:"
    )

def greedy_generate(prompt: str, max_new_tokens: int = 16, use_ablation: bool = False) -> str:
    toks = model.to_tokens(prompt)
    if use_ablation:
        with model.hooks(fwd_hooks=ABLT_HOOKS):
            out = model.generate(toks, max_new_tokens=max_new_tokens)
    else:
        out = model.generate(toks, max_new_tokens=max_new_tokens)
    gen = model.to_string(out[0, toks.shape[1]:])
    return gen.strip()

def evaluate_single_example(row: pd.Series, use_ablation: bool, targets: List[Tuple[int,int]]) -> Dict[str, Any]:
    prompt = build_prompt(row)
    answer = greedy_generate(prompt, max_new_tokens=16, use_ablation=use_ablation)
    halluc_label = 0  # scorer comes later; we export answers for external scoring
    return {
        "row_id": int(row["row_id"]),
        "hallucinated": int(halluc_label),
        "entropy": np.nan,
        "erank":   np.nan,
        "hsim":    np.nan,
        "self_ratio": np.nan,
        "answer": answer
    }

def main():
    plan_id_col = plan_df.columns[0]
    plan_ids = set(plan_df[plan_id_col].tolist())
    subset = ex_df[ex_df["row_id"].isin(plan_ids)].copy().sort_values("row_id")

    rows = []
    for _, row in subset.iterrows():
        base  = evaluate_single_example(row, use_ablation=False, targets=[])
        after = evaluate_single_example(row, use_ablation=True,  targets=TARGETS)
        rows.append({
            "row_id": int(row["row_id"]),
            "halluc_before": int(base["hallucinated"]),
            "halluc_after":  int(after["hallucinated"]),
            "d_entropy":    np.nan,
            "d_erank":      np.nan,
            "d_hsim":       np.nan,
            "d_self_ratio": np.nan,
            "answer_before": base.get("answer"),
            "answer_after":  after.get("answer")
        })

    out = pd.DataFrame(rows)
    OUTPUT.parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(OUTPUT, index=False)
    print(f"[OK] Saved paired ablation results -> {OUTPUT}  (n={len(out)})")

if __name__ == "__main__":
    main()
'''

with open(f"{STEP5}/step5_ablation_runner.py","w") as f:
    f.write(runner_code)

print("Runner written:", f"{STEP5}/step5_ablation_runner.py")


%cd /content/drive/MyDrive/attention-collapse-llm/step5
!python step5_ablation_runner.py


import pandas as pd
from scipy.stats import binomtest
import re
from collections import Counter

res_path = f"{OUT}/step5_ablation_results.csv"
ex_path  = f"{STEP5}/merged_predictions_metrics.csv"

res = pd.read_csv(res_path)
ex  = pd.read_csv(ex_path)[["row_id","gold_answer"]]
df  = res.merge(ex, on="row_id", how="left")

def normalize_text_strict(s: str) -> str:
    s = (s or "").lower().strip()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

def f1_match(pred: str, gold: str) -> float:
    p = normalize_text_strict(pred).split()
    g = normalize_text_strict(gold).split()
    if not p or not g: return 0.0
    pc, gc = Counter(p), Counter(g)
    overlap = sum((pc & gc).values())
    if overlap == 0: return 0.0
    precision = overlap / len(p)
    recall    = overlap / len(g)
    return 2 * precision * recall / (precision + recall + 1e-9)

df["halluc_before_f1"] = df.apply(lambda r: 1 if f1_match(str(r.get("answer_before","")) or "", str(r.get("gold_answer","")) or "") < 0.5 else 0, axis=1)
df["halluc_after_f1"]  = df.apply(lambda r: 1 if f1_match(str(r.get("answer_after",""))  or "", str(r.get("gold_answer","")) or "") < 0.5 else 0, axis=1)

before = df["halluc_before_f1"].mean()
after  = df["halluc_after_f1"].mean()
delta  = after - before

b01 = ((df["halluc_before_f1"]==0) & (df["halluc_after_f1"]==1)).sum()
b10 = ((df["halluc_before_f1"]==1) & (df["halluc_after_f1"]==0)).sum()
n   = b01 + b10
pval = binomtest(k=min(b01,b10), n=n, p=0.5, alternative="two-sided").pvalue if n>0 else float("nan")

print(f"Hallucination (F1<0.5) — before: {before:.3f} | after: {after:.3f} | delta: {delta:.3f}")
print(f"McNemar: 0→1={b01}, 1→0={b10}, pairs={n}, p={pval:.4g}")

scored_path = f"{OUT}/step5_ablation_results_scored.csv"
df.to_csv(scored_path, index=False)
print("Saved scored file:", scored_path)

# quick peek
df.head(5)


# ===== Step A — Build master analysis table =====
import os, re, json
import pandas as pd
from collections import Counter

ROOT  = "/content/drive/MyDrive/attention-collapse-llm"
STEP5 = f"{ROOT}/step5"
OUT   = f"{ROOT}/step5_outputs"

paths = {
    "res":   f"{OUT}/step5_ablation_results.csv",          # from runner (has answer_before/answer_after)
    "ex":    f"{STEP5}/merged_predictions_metrics.csv",    # examples + gold_answer
    "qa":    f"{STEP5}/Distractors - Sheet1.csv"           # synthetic QA sheet (for metadata)
}

for k,p in paths.items():
    assert os.path.exists(p), f"Missing {k}: {p}"

# --- load
res = pd.read_csv(paths["res"])
ex  = pd.read_csv(paths["ex"])
qa  = pd.read_csv(paths["qa"])

# --- normalize/join keys
ex["question"] = ex["question"].astype(str).str.strip()
qa["Question_norm"] = qa["Question"].astype(str).str.strip() if "Question" in qa.columns else ""
# Merge ablation results (on row_id) -> examples (on row_id)
m = res.merge(ex, on="row_id", how="left", validate="one_to_one")

# --- Join QA metadata by question text (robust, only uses columns that exist)
# Normalize a clean "question" on the QA sheet
if "Question" in qa.columns:
    qa["Question_norm"] = qa["Question"].astype(str).str.strip()
else:
    qa["Question_norm"] = ""  # if sheet lacks the column, will be ignored

# Map SOURCE column names -> TARGET names for a tidy qa_meta
col_map = {
    "interference_type": ["Interference Type","interference_type","Interference"],
    "distractor_density": ["Distractor Density","distractor_density","Distractors"],
    "evidence_position": ["Evidence Position","evidence_position","Evidence Location"],
    "context_length": ["Context Length","context_length","Ctx Len"],
}

source_to_target = {"Question_norm": "question"}  # <- create 'question' from 'Question_norm'
found_targets = []  # which metadata targets we actually found

for target_name, candidates in col_map.items():
    for cand in candidates:
        if cand in qa.columns:
            source_to_target[cand] = target_name
            found_targets.append(target_name)
            break

# Build qa_meta with only the columns that exist
qa_meta = qa.rename(columns=source_to_target)[["question"] + found_targets] if "Question_norm" in qa.columns else pd.DataFrame()

# Merge onto the main table by 'question' if available
if not qa_meta.empty and "question" in qa_meta.columns and "question" in m.columns:
    m = m.merge(qa_meta, on="question", how="left")


# --- F1 scorer (strict-ish but reasonable)
def norm(s: str) -> str:
    s = (s or "").lower().strip()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s
def f1_match(pred: str, gold: str) -> float:
    p = norm(pred).split()
    g = norm(gold).split()
    if not p or not g: return 0.0
    pc, gc = Counter(p), Counter(g)
    overlap = sum((pc & gc).values())
    if overlap == 0: return 0.0
    precision = overlap / len(p)
    recall    = overlap / len(g)
    return 2 * precision * recall / (precision + recall + 1e-9)

# prefer answers produced by runner; fallback to model_answer if needed
m["answer_before_eff"] = m["answer_before"].fillna(m.get("model_answer", ""))
m["answer_after_eff"]  = m["answer_after"].fillna(m.get("model_answer", ""))

# ensure gold exists
if "gold_answer" not in m.columns:
    raise RuntimeError("gold_answer missing in merged_predictions_metrics.csv — please merge from QA first.")

# halluc flags (1 = hallucination)
m["halluc_before_f1"] = m.apply(lambda r: 1 if f1_match(str(r["answer_before_eff"]), str(r["gold_answer"])) < 0.5 else 0, axis=1)
m["halluc_after_f1"]  = m.apply(lambda r: 1 if f1_match(str(r["answer_after_eff"]),  str(r["gold_answer"])) < 0.5 else 0, axis=1)
m["improved"]         = ((m["halluc_before_f1"]==1) & (m["halluc_after_f1"]==0)).astype(int)
m["worsened"]         = ((m["halluc_before_f1"]==0) & (m["halluc_after_f1"]==1)).astype(int)

# quick sanity prints
n = len(m)
before = m["halluc_before_f1"].mean()
after  = m["halluc_after_f1"].mean()
print(f"n={n} | before={before:.3f} | after={after:.3f} | delta={(after-before):.3f}")
print("columns present:", [c for c in ["interference_type","distractor_density","evidence_position","context_length"] if c in m.columns])

# save master
master_path = f"{OUT}/step5_analysis_master.csv"
m.to_csv(master_path, index=False)
print("saved master:", master_path)

# peek a few rows
display(m[["row_id","interference_type","distractor_density","evidence_position","halluc_before_f1","halluc_after_f1","improved","worsened"]].head(8) if "interference_type" in m.columns or "distractor_density" in m.columns else m.head(8))


import pandas as pd

master_path = "/content/drive/MyDrive/attention-collapse-llm/step5_outputs/step5_analysis_master.csv"
m = pd.read_csv(master_path)
print("rows:", len(m))
print("columns:", list(m.columns)[:15], "...")


# Hallucination rates before/after
print("Hallucination (F1<0.5) — before=%.3f | after=%.3f" % (
    (m["halluc_before_f1"]<0.5).mean(),
    (m["halluc_after_f1"]<0.5).mean()
))

# How many improved vs worsened
print("Improved:", (m["improved"]==1).sum(), "| Worsened:", (m["worsened"]==1).sum())

# If metadata columns exist (like interference_type, distractor_density)
for col in ["interference_type","distractor_density","evidence_position","context_length"]:
    if col in m.columns:
        print("\nBy", col)
        print(m.groupby(col)[["halluc_before_f1","halluc_after_f1"]].mean())


import matplotlib.pyplot as plt

# Histogram of entropy/rank deltas
for col in ["d_entropy","d_erank","d_hsim"]:
    if col in m.columns:
        m[col].dropna().hist(bins=20)
        plt.title(col + " distribution")
        plt.show()

# Scatterplot: entropy change vs hallucination
if "d_entropy" in m.columns:
    plt.scatter(m["d_entropy"], (m["halluc_after_f1"]<0.5).astype(int))
    plt.xlabel("Δ Entropy")
    plt.ylabel("Hallucination after (0/1)")
    plt.show()
